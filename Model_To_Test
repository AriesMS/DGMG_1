import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.distributions import Bernoulli, Categorical
import json
import dgl
from dgl import batch
from torch.utils.data import DataLoader
from collections import Counter
import numpy as np

# Define dataset and collate function
def collate(samples):
    graphs, labels, train_masks, val_masks, test_masks = zip(*samples)
    batched_graph = dgl.batch(graphs)
    batched_labels = torch.tensor(labels)
    batched_train_masks = torch.stack(train_masks)
    batched_val_masks = torch.stack(val_masks)
    batched_test_masks = torch.stack(test_masks)
    return batched_graph, batched_labels, batched_train_masks, batched_val_masks, batched_test_masks

# Load the dataset
with open('D:/Docs/New/RA/Models/DGMG_TEST/BuildingFootprints_Normalised_24k.json', 'r') as f:
    raw_data = json.load(f)

# Count the number of graphs for each program
program_counts = Counter()
for graph_data in raw_data.values():
    for typology in graph_data["LU_DESC_Typology"]:
        program_counts[typology] += 1

# Filter for residential graphs
residential_data = {k: v for k, v in raw_data.items() if "RESIDENTIAL_residential" in v["LU_DESC_Typology"]}

unique_labels = set()
for graph_data in residential_data.values():
    for typology in graph_data["LU_DESC_Typology"]:
        unique_labels.add(typology)

class CustomGraphDataset:
    def __init__(self, data_list):
        self.data_list = data_list

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        graph_data = self.data_list[idx]

        # Create edge_index tensor
        edge_index = torch.tensor(graph_data["TopologyList"], dtype=torch.long).t().contiguous()

        # Create vertices tensor
        x = torch.tensor(graph_data["Vertices"], dtype=torch.float)

        # Process labels
        y = torch.tensor([label_mapping[graph_data["LU_DESC_Typology"][0]]], dtype=torch.long)

        # Create a DGL graph
        g = dgl.graph((edge_index[0], edge_index[1]))
        g.ndata['hv'] = x  # hv is the node feature

        # Create masks
        num_nodes = x.size(0)
        train_mask = torch.zeros(num_nodes, dtype=torch.bool)
        val_mask = torch.zeros(num_nodes, dtype=torch.bool)
        test_mask = torch.zeros(num_nodes, dtype=torch.bool)

        indices = np.random.permutation(num_nodes)
        train_size = int(0.6 * num_nodes)
        val_size = int(0.2 * num_nodes)

        train_mask[indices[:train_size]] = True
        val_mask[indices[train_size:train_size + val_size]] = True
        test_mask[indices[train_size + val_size:]] = True

        return g, y, train_mask, val_mask, test_mask

label_mapping = {label: i for i, label in enumerate(unique_labels)}

dataset = CustomGraphDataset(list(residential_data.values()))
data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate)

# Define action log probability function
def bernoulli_action_log_prob(logit, action):
    action_tensor = torch.tensor(action, dtype=torch.float32)
    return Bernoulli(logits=logit).log_prob(action_tensor)

class AddNode(nn.Module):
    def __init__(self, v_max, node_hidden_size):
        super(AddNode, self).__init__()
        self.linear = nn.Linear(node_hidden_size, 1)
        self.log_prob = []
        self.node_hidden_size = node_hidden_size

    def _initialize_node_repr(self, g):
        default_node_repr = torch.zeros(1, self.node_hidden_size)
        if 'hv' in g.ndata:
            print(f"Before concatenation - g.ndata['hv']: {g.ndata['hv'].shape}, default_node_repr: {default_node_repr.shape}")  # Debugging print
            g.ndata['hv'] = torch.cat([g.ndata['hv'], default_node_repr], dim=0)
            print(f"After concatenation - g.ndata['hv']: {g.ndata['hv'].shape}")  # Debugging print
        else:
            g.ndata['hv'] = default_node_repr
        print(f"Initialized node repr - g.ndata['hv']: {g.ndata['hv']}")  # Debugging print

    def forward(self, g, action=None):
        print(f"Before adding node - g.ndata['hv']: {g.ndata['hv'].shape}, g.num_nodes(): {g.num_nodes()}")  # Debugging print
        if g.num_nodes() > 0:
            last_node_feature = g.ndata['hv'][-1]
        else:
            last_node_feature = torch.zeros(self.node_hidden_size)

        logit = self.linear(last_node_feature)
        prob = torch.sigmoid(logit)

        if not self.training:
            action = Bernoulli(prob).sample().item()

        stop = bool(action == 1)
        if not stop:
            g.add_nodes(1)
            self._initialize_node_repr(g)
            print(f"After adding node - g.ndata['hv']: {g.ndata['hv'].shape}, g.num_nodes(): {g.num_nodes()}")  # Debugging print

        if self.training:
            sample_log_prob = bernoulli_action_log_prob(logit, action)
            self.log_prob.append(sample_log_prob)

        return stop

class AddEdge(nn.Module):
    def __init__(self, node_hidden_size):
        super(AddEdge, self).__init__()
        self.add_edge = nn.Linear(node_hidden_size, 1)
        self.log_prob = []

    def forward(self, g, action=None):
        if g.num_nodes() > 0:
            last_node_feature = g.ndata['hv'][-1]
        else:
            last_node_feature = torch.zeros(self.node_hidden_size)

        logit = self.add_edge(last_node_feature)
        prob = torch.sigmoid(logit)

        if not self.training:
            action = Bernoulli(prob).sample().item()

        to_add_edge = bool(action == 0)
        if self.training:
            sample_log_prob = bernoulli_action_log_prob(logit, action)
            self.log_prob.append(sample_log_prob)

        return to_add_edge

class ChooseDestAndUpdate(nn.Module):
    def __init__(self, node_hidden_size):
        super(ChooseDestAndUpdate, self).__init__()
        self.choose_dest = nn.Linear(2 * node_hidden_size, 1)
        self.log_prob = []

    def forward(self, g, dest=None):
        src = g.num_nodes() - 1
        possible_dests = range(src)

        src_embed_expand = g.ndata['hv'][src].expand(src, -1)
        possible_dests_embed = g.ndata['hv'][possible_dests]

        dests_scores = self.choose_dest(torch.cat([possible_dests_embed, src_embed_expand], dim=1)).view(1, -1)
        dests_probs = F.softmax(dests_scores, dim=1)

        if not self.training:
            dest = Categorical(dests_probs).sample().item()

        if not g.has_edges_between(src, dest):
            src_list = [src, dest]
            dest_list = [dest, src]
            g.add_edges(src_list, dest_list)

        if self.training and dests_probs.nelement() > 1:
            self.log_prob.append(F.log_softmax(dests_scores, dim=1)[:, dest:dest + 1])

class PolicyNetwork(nn.Module):
    def __init__(self, node_hidden_size):
        super(PolicyNetwork, self).__init__()
        self.add_node = nn.Linear(node_hidden_size, 1)
        self.add_edge = nn.Linear(node_hidden_size, 1)
        self.choose_dest = nn.Linear(2 * node_hidden_size, 1)

    def forward(self, graph):
        print(f"Graph node features before accessing 'hv' - graph.ndata: {graph.ndata}")  # Debugging print
        last_node_feature = graph.ndata['hv'][-1]
        print(f"Last node feature: {last_node_feature}")  # Debugging print

        add_node_logit = self.add_node(last_node_feature)
        add_node_prob = torch.sigmoid(add_node_logit)

        add_edge_logit = self.add_edge(last_node_feature)
        add_edge_prob = torch.sigmoid(add_edge_logit)

        return add_node_prob, add_edge_prob

    def choose_destination(self, graph):
        src = graph.num_nodes() - 1
        possible_dests = range(src)

        src_embed_expand = graph.ndata['hv'][src].expand(src, -1)
        possible_dests_embed = graph.ndata['hv'][possible_dests]

        dests_scores = self.choose_dest(torch.cat([possible_dests_embed, src_embed_expand], dim=1)).view(1, -1)
        dests_probs = F.softmax(dests_scores, dim=1)

        return dests_probs

class DGMG(nn.Module):
    def __init__(self, v_max, node_hidden_size, num_prop_rounds):
        super(DGMG, self).__init__()
        self.v_max = v_max
        self.node_hidden_size = node_hidden_size
        self.add_node_agent = AddNode(v_max, node_hidden_size)
        self.add_edge_agent = AddEdge(node_hidden_size)
        self.choose_dest_agent = ChooseDestAndUpdate(node_hidden_size)
        self.policy_net = PolicyNetwork(node_hidden_size)

    def add_node_and_update(self, action=None):
        return self.add_node_agent(self.g, action)

    def add_edge_or_not(self, action=None):
        return self.add_edge_agent(self.g, action)

    def choose_dest_and_update(self, action=None):
        self.choose_dest_agent(self.g, action)

    def get_log_prob(self):
        log_probs = torch.cat(self.add_node_agent.log_prob + self.add_edge_agent.log_prob + self.choose_dest_agent.log_prob)
        return log_probs.sum()

    def forward_train(self):
        self.g = dgl.graph(([], []))
        self.g.set_n_initializer(dgl.frame.zero_initializer)
        self.g.set_e_initializer(dgl.frame.zero_initializer)

        self.add_node_agent.log_prob = []
        self.add_edge_agent.log_prob = []
        self.choose_dest_agent.log_prob = []

        print(f"Initialized graph - g.ndata before adding any nodes: {self.g.ndata}")  # Debugging print

        for _ in range(self.v_max):  # Limit the number of steps to v_max
            if self.g.num_nodes() == 0:
                self.g.add_nodes(1)
                self.g.ndata['hv'] = torch.zeros(1, self.node_hidden_size)
                print(f"Graph node features after adding the first node - g.ndata['hv']: {self.g.ndata['hv']}")  # Debugging print

            add_node_prob, add_edge_prob = self.policy_net(self.g)
            print(f"Graph node features before action - g.ndata: {self.g.ndata}")  # Debugging print

            add_node_action = Bernoulli(add_node_prob).sample().item()
            stop = self.add_node_and_update(add_node_action)
            if stop:
                break

            add_edge_action = Bernoulli(add_edge_prob).sample().item()
            if add_edge_action == 1:
                dest_probs = self.policy_net.choose_destination(self.g)
                dest_node = Categorical(dest_probs).sample().item()
                self.g.add_edges(self.g.num_nodes() - 1, dest_node)
                self.g.add_edges(dest_node, self.g.num_nodes() - 1)

        return self.get_log_prob()

    def forward_inference(self):
        stop = self.add_node_and_update()
        while not stop:
            num_trials = 0
            to_add_edge = self.add_edge_or_not()
            while to_add_edge and (num_trials < self.g.num_nodes() - 1):
                self.choose_dest_and_update()
                num_trials += 1
                to_add_edge = self.add_edge_or_not()
            stop = self.add_node_and_update()
        return self.g

    def forward(self, actions=None):
        if self.training:
            return self.forward_train()
        else:
            return self.forward_inference()

v_max = 20  # max num of nodes
node_hidden_size = 128
num_prop_rounds = 3

model = DGMG(v_max, node_hidden_size, num_prop_rounds)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def reward_function(generated_graph, target_graph):
    return 1.0 if generated_graph.number_of_edges() == target_graph.number_of_edges() else 0.0

def train_step(graph, target_graph):
    model.train()
    optimizer.zero_grad()

    print(f"Train Step - graph.ndata['hv']: {graph.ndata['hv'].shape}, graph.num_nodes(): {graph.num_nodes()}")  # Debugging print
    log_prob = model()
    loss = -log_prob
    loss.backward()
    optimizer.step()

    return loss.item()

# Training
for epoch in range(100):
    batch_loss = 0
    for data in data_loader:
        g, y, train_mask, val_mask, test_mask = data
        print(f"Epoch {epoch+1} - Initial graph.ndata['hv']: {g.ndata['hv'].shape}, num_nodes: {g.num_nodes()}")  # Debugging print
        loss = train_step(g, target_graph=g)
        batch_loss += loss

    print(f'Epoch {epoch+1}, Loss: {batch_loss / len(data_loader)}')